{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-object-detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPVBdhJK0AGK4fn1XebGxmT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/pytorch-computer-vision-cookbook/blob/main/5-multi-object-detection/multi_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYnu9PNv3NW"
      },
      "source": [
        "# Multi-Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8iTz06Xv96_"
      },
      "source": [
        "Object detection is the process of locating and classifying existing objects in an image. Identified objects are shown with bounding boxes in the image. There are two methods for general object detection: region proposal-based and regression/classification-based. \r\n",
        "\r\n",
        "In this notebook, we will use a regression/classification-based method called YOLO.we will learn how to implement the YOLO-v3 algorithm and train and\r\n",
        "deploy it for object detection using PyTorch.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b08H4QZ1x5Mh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERXmfyIgx6hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d303585b-4f25-4e2c-f489-8f14f3a4e27f"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torchvision.transforms.functional as TF\r\n",
        "from torchvision.transforms.functional import to_pil_image\r\n",
        "from torch import optim\r\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\r\n",
        "\r\n",
        "\r\n",
        "import torch\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(torch.__version__)\r\n",
        "\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "\r\n",
        "import copy\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pylab as plt\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcx0rArMwN4K"
      },
      "source": [
        "## Creating datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8OY4pK8wPF4"
      },
      "source": [
        "We will need to download the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAX1F7s3yfDG"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "# Download the following GitHub repository\r\n",
        "git clone https://github.com/pjreddie/darknet\r\n",
        "\r\n",
        "# Create a folder named data\r\n",
        "mkdir data\r\n",
        "\r\n",
        "# copy the get_coco_dataset.sh file\r\n",
        "cp darknet/scripts/get_coco_dataset.sh data\r\n",
        "\r\n",
        "# execute the get_coco_dataset.sh file\r\n",
        "chmod 755 data/get_coco_dataset.sh\r\n",
        "./data/get_coco_dataset.sh\r\n",
        "\r\n",
        "# Create a folder named config\r\n",
        "mkdir data/config\r\n",
        "# copy the yolov3.cfg file\r\n",
        "cp darknet/cfg/yolov3.cfg data/config/\r\n",
        "\r\n",
        "# Finally, download the coco.names file and put it in the data folder\r\n",
        "wget https://github.com/pjreddie/darknet/blob/master/data/coco.names\r\n",
        "cp coco.names data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHVVh1wl5S8W"
      },
      "source": [
        "### Creating a custom COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7n523yg5U9B"
      },
      "source": [
        "Now that we've downloaded the COCO dataset, we will create training and validation datasets and dataloaders using PyTorch's Dataset and Dataloader classes.\r\n",
        "\r\n",
        "we will define the CocoDataset class and show some sample images from\r\n",
        "the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS3CdwTyz4kQ"
      },
      "source": [
        "class CocoDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, files_path, transform=None, trans_params=None):\r\n",
        "    # get list of images\r\n",
        "    with opne(files_path, \"r\") as file:\r\n",
        "      self.img_path = file.readlines()\r\n",
        "    # get list of labels\r\n",
        "    self.label_path = [path.replace(\"images\", \"labels\").replace(\".png\", \"txt\").replace(\".jpg\", \".txt\") for path in self.img_path]\r\n",
        "    self.trans_params = trans_params \r\n",
        "    self.transform = transform \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.img_path)\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    img_path = self.img_path[index % len(self.img_path)].rstrip()\r\n",
        "    img = Image.open(img_path).convert(\"RGB\")\r\n",
        "    label_path = self.label_path[index % len(self.img_path)].rstrip()\r\n",
        "\r\n",
        "    labels = None\r\n",
        "    if os.path.exists(label_path):\r\n",
        "      labels = np.loadtxt(label_path).replace(-1, 5)\r\n",
        "    if self.transform:\r\n",
        "      img, labels = self.transform(img, labels, self.trans_params)\r\n",
        "\r\n",
        "    return img, labels, img_path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtPZ47Zq58l1"
      },
      "source": [
        "Next, we will create an object of the CocoDataset class for the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bla3nDy84ett"
      },
      "source": [
        "root_data = \"./data/coco\"\r\n",
        "train_file_path = os.path.join(root_data, \"trainvalno5k.txt\")\r\n",
        "coco_train = CocoDataset(train_file_path)\r\n",
        "print(len(coco_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQEhrvjA6l2r"
      },
      "source": [
        "# Get a sample item from coco_val:\r\n",
        "img, labels, img_path = coco_train[1] \r\n",
        "print(\"image size:\", img.size, type(img))\r\n",
        "print(\"labels shape:\", labels.shape, type(labels))\r\n",
        "print(\"labels \\n\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnikLO-r6zVh"
      },
      "source": [
        "Let's display a sample image from the coco_train and coco_val datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYeV0rUw60LL"
      },
      "source": [
        "val_file_path = os.path.join(root_data, \"5k.txt\")\r\n",
        "coco_val = CocoDataset(val_file_path, transform=None, trans_params=None)\r\n",
        "print(len(coco_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7fdGIVLBoAA"
      },
      "source": [
        "# Get a sample item from coco_val:\r\n",
        "img, labels, img_path = coco_val[7] \r\n",
        "print(\"image size:\", img.size, type(img))\r\n",
        "print(\"labels shape:\", labels.shape, type(labels))\r\n",
        "print(\"labels \\n\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE2Z2pSOB75K"
      },
      "source": [
        "Let's display a sample image from the coco_train and coco_val datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llLvc96UB8xN"
      },
      "source": [
        "# Get a list of COCO object names\r\n",
        "coco_names_path=\"./data/coco.names\"\r\n",
        "fp = open(coco_names_path, \"r\")\r\n",
        "coco_names = fp.read().split(\"\\n\")[:-1]\r\n",
        "print(\"number of classese:\", len(coco_names))\r\n",
        "print(coco_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_PWY1bhCUa0"
      },
      "source": [
        "# Define a rescale_bbox helper function to rescale normalized bounding boxes to the original image size\r\n",
        "def rescale_bbox(bb, W, H):\r\n",
        "  x, y, w, h = bb\r\n",
        "  return [x * W, y * H, w * W, h * H]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZZT7wK3Cwlo"
      },
      "source": [
        "# Define the show_img_bbox helper function to show an image with object bounding boxes\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "# if the font that's passed to ImageFont.truetype is not available\r\n",
        "# Alternatively, you may use a more common font\r\n",
        "# fnt = ImageFont.truetype('arial.ttf', 16)\r\n",
        "fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 16)\r\n",
        "\r\n",
        "def show_img_bbox(img, targets):\r\n",
        "  if torch.is_tensor(img):\r\n",
        "      img=to_pil_image(img)\r\n",
        "  if torch.is_tensor(targets):\r\n",
        "      targets=targets.numpy()[:,1:]\r\n",
        "      \r\n",
        "  W, H=img.size\r\n",
        "  draw = ImageDraw.Draw(img)\r\n",
        "  \r\n",
        "  for target in targets:\r\n",
        "      id_=int(target[0])\r\n",
        "      bbox=target[1:]\r\n",
        "      bbox=rescale_bbox(bbox,W,H)\r\n",
        "      xc, yc, w, h=bbox\r\n",
        "      \r\n",
        "      color = [int(c) for c in COLORS[id_]]\r\n",
        "      name=coco_names[id_]\r\n",
        "      \r\n",
        "      draw.rectangle(((xc-w/2, yc-h/2), (xc+w/2, yc+h/2)), outline=tuple(color), width=3)\r\n",
        "      draw.text((xc-w/2, yc-h/2), name, font=fnt, fill=(255, 255, 255, 0))\r\n",
        "  plt.imshow(np.array(img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGHUB83DVdE"
      },
      "source": [
        "# Call the show_img_bbox helper function to show a sample image from coco_train\r\n",
        "np.random.seed(2)\r\n",
        "rnd_ind=np.random.randint(len(coco_train))\r\n",
        "img, labels, img_path = coco_train[rnd_ind] \r\n",
        "print(img.size, labels.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "show_img_bbox(img, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTnFFEmEJBO"
      },
      "source": [
        "# Call the show_img_bbox helper function to show a sample image from coco_val\r\n",
        "np.random.seed(0)\r\n",
        "rnd_ind=np.random.randint(len(coco_val))\r\n",
        "img, labels, img_path = coco_val[rnd_ind] \r\n",
        "print(img.size, labels.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "show_img_bbox(img, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57j3VGL1EIea"
      },
      "source": [
        "### Transforming data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx1yM6tZEcMM"
      },
      "source": [
        "In this section, we will define a transform function and the parameters to be passed to the CocoDataset class.\r\n",
        "\r\n",
        "In the Transforming the data subsection, we defined the functions required for data transformation. These transformations were required to resize images, augment the data, or convert the data into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8yfXZqiwXaG"
      },
      "source": [
        "# First, we will define a pad_to_square helper function\r\n",
        "def pad_to_square(img, boxes, pad_value=0, normalized_labels=True):\r\n",
        "  \"\"\"\r\n",
        "  img: A PIL image\r\n",
        "  boxes: A numpy array with a shape of (n, 5) that contains n bounding boxes\r\n",
        "  pad_value: The pixel fill value, which defaults to zero\r\n",
        "  normalized_labels: A flag to show whether the bounding boxes were normalized to the range [0, 1]\r\n",
        "  \"\"\"\r\n",
        "  w, h = img.size\r\n",
        "  w_factor, h_factor = (w, h) if normalized_labels else (1, 1)\r\n",
        "\r\n",
        "  # calculate the padding size and divided it into two values: pad1 and pad2\r\n",
        "  dim_diff = np.abs(h - w)\r\n",
        "  pad1 = dim_diff // 2\r\n",
        "  pad2 = dim_diff - pad1\r\n",
        "\r\n",
        "  if h <= w:\r\n",
        "    left, top, right, bottom = 0, pad1, 0, pad2\r\n",
        "  else:\r\n",
        "    left, top, right, bottom = pad1, 0, pad2, 0\r\n",
        "  padding = (left, top, right, bottom)\r\n",
        "\r\n",
        "  # calculate the padding size on each side of the image\r\n",
        "  img_padded = TF.pad(img, padding=padding, fill=pad_value)\r\n",
        "  w_padded, h_padded = img_padded.size\r\n",
        "\r\n",
        "  # adjust the bounding box coordinates based on the padding size.\r\n",
        "  x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\r\n",
        "  y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\r\n",
        "  x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\r\n",
        "  y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\r\n",
        "\r\n",
        "  # Then, we adjusted x1, y1, x2, y2 by adding the padding sizes.\r\n",
        "  x1 += padding[0]   # left\r\n",
        "  y1 += padding[1]   # top\r\n",
        "  x2 += padding[2]   # right\r\n",
        "  y2 += padding[3]   # bottom\r\n",
        "\r\n",
        "  # calculate the bounding boxes using the adjusted values of x1, y1, x2, y2.\r\n",
        "  # Note that we normalized the labels again to the range of [0, 1].\r\n",
        "  boxes[:, 1] = ((x1 + x2) / 2) / w_padded\r\n",
        "  boxes[:, 2] = ((y1 + y2) / 2) / h_padded\r\n",
        "  boxes[:, 3] *= w_factor / w_padded\r\n",
        "  boxes[:, 4] *= h_factor / h_padded\r\n",
        "\r\n",
        "  return img_padded, boxes"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifVuwxT5da9"
      },
      "source": [
        "# Define the hflip helper function to horizontally flip images\r\n",
        "def hflip(image, labels):\r\n",
        "  image = TF.hflip(image)\r\n",
        "  labels[:, 1] = 1.0 - labels[:, 1]\r\n",
        "\r\n",
        "  return image, labels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgr2HH0Z6EvB"
      },
      "source": [
        "# Define the transformer function\r\n",
        "def transformer(image, labels, params):\r\n",
        "  \"\"\"\r\n",
        "  image: A PIL image\r\n",
        "  labels: Bounding boxes as a numpy array that's (n, 5) in size\r\n",
        "  params: A Python dictionary containing the transformation parameters\r\n",
        "  \"\"\"\r\n",
        "  if params[\"pad2square\"] is True:\r\n",
        "    image, labels = pad_to_square(image, labels)\r\n",
        "  image = TF.resize(image, params[\"target_size\"])\r\n",
        "\r\n",
        "  if random.random() < params[\"p_hflip\"]:\r\n",
        "    image, labels = hflip(image, labels)   # randomly flip the image for data augmentation\r\n",
        "\r\n",
        "  image = TF.to_tensor(image)              # convert the PIL image into a PyTorch tensor\r\n",
        "  targets = torch.zeros((len(labels), 6))  # also convert into a PyTorch tensor of size n*6. The extra dimension will be used to index images in a mini-batch.\r\n",
        "  targets[:, 1:] = torch.from_numpy(labels)\r\n",
        "\r\n",
        "  return image, targets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTa1PS-38yCu"
      },
      "source": [
        "We redefined `coco_train`; however, this time, we passed transformer and\r\n",
        "`trans_params_train` to the CocoDataset class. To force the horizontal flip, we set the `p_hflip` probability to 1.0. In practice, we usually set the probability to 0.5. You can see the effect of the transformations on the sample image. The image has been zero-padded from the top and bottom, resized to 416*416, and horizontally flipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIo2I-xZ9E8P"
      },
      "source": [
        "# Now, let's create an object of CocoDataset for training data by passing the transformer\r\n",
        "trans_params_train = {\r\n",
        "    \"target_size\": (416, 416),\r\n",
        "    \"pad2square\": True,\r\n",
        "    \"p_hflip\": 1.0,\r\n",
        "    \"normalized_labels\": True\r\n",
        "}\r\n",
        "\r\n",
        "coco_train = CocoDataset(train_file_path, transform=transformer, trans_params=trans_params_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHRU4Mq9AlfS"
      },
      "source": [
        "np.random.seed(2)\r\n",
        "rnd_ind = np.random.randint(len(coco_train))\r\n",
        "img, targets, img_path = coco_train[rnd_ind]\r\n",
        "print(\"image shape:\", img.shape)\r\n",
        "print(\"labels shape:\", targets.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "show_img_bbox(img,targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHgRd-DQ9KqQ"
      },
      "source": [
        "Similarly, we redefined coco_val. We did not need data augmentation for the\r\n",
        "validation data, so we set the probability of p_hflip to 0.0. Check out the transformed sample size. It has been zero-padded from the top and bottom and resized to 416*416 but not flipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LxQkQnA9OFX"
      },
      "source": [
        "# Similarly, we will define an object of CocoDataset by passing the transformer to validate the data\r\n",
        "trans_params_val = {\r\n",
        "    \"target_size\": (416, 416),\r\n",
        "    \"pad2square\": True,\r\n",
        "    \"p_hflip\": 0.0,\r\n",
        "    \"normalized_labels\": True\r\n",
        "}\r\n",
        "\r\n",
        "coco_val = CocoDataset(val_file_path, transform=transformer, trans_params=trans_params_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jz9uH6TBjPG"
      },
      "source": [
        "np.random.seed(2)\r\n",
        "rnd_ind = np.random.randint(len(coco_val))\r\n",
        "img, targets, img_path = coco_val[rnd_ind]\r\n",
        "print(\"image shape:\", img.shape)\r\n",
        "print(\"labels shape:\", targets.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "show_img_bbox(img,targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTsLgmr0Bu7-"
      },
      "source": [
        "### Defining the Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij0t73SaB0WC"
      },
      "source": [
        "We will define two dataloaders for training and validation of datasets so we\r\n",
        "can get mini-batches of data from coco_train and coco_val.\r\n",
        "\r\n",
        "We also defined the collate_fn function to process a mini-batch and return\r\n",
        "PyTorch tensors. The function was given as an argument to the Dataloader class so that the process happens on the fly. In the function, we grouped the images, targets, and paths in the mini-batch using zip(*iterateble). Then, we removed any empty bounding boxes in the targets. Next, we set the sample index in the mini-batch. Finally, we concatenated the images and targets as PyTorch tensors. To see how this works, we extracted a mini-batch from train_dl and val_dl and printed the shape of the returned tensors.\r\n",
        "\r\n",
        "Define an object of the Dataloader class for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPPoXhIZ-I_C"
      },
      "source": [
        "batch_size = 8\r\n",
        "\r\n",
        "def collate_fn(batch):\r\n",
        "  imgs, targets, paths = list(zip(*batch))\r\n",
        "\r\n",
        "  # Remove empty boxes\r\n",
        "  targets = [boxes for boxes in targets if boxes is not None]\r\n",
        "\r\n",
        "  # set the sample index\r\n",
        "  for b_i, boxes in enumerate(targets):\r\n",
        "    boxes[:, 0] = b_i\r\n",
        "  targets = torch.cat(targets, 0)\r\n",
        "  imgs = torch.stack([img for img in imgs])\r\n",
        "\r\n",
        "  return imgs, targets, paths"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RPdwMIp--BS"
      },
      "source": [
        "train_dataloader = DataLoader(coco_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsH6qhr1_aBK"
      },
      "source": [
        "Let's extract a mini-batch from train_dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b92tAyO_cUc"
      },
      "source": [
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "for imgs_batch, target_batch, path_batch in train_dataloader:\r\n",
        "  break\r\n",
        "\r\n",
        "print(imgs_batch.shape)\r\n",
        "print(target_batch.shape, target_batch.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz7cYPVuACbg"
      },
      "source": [
        "Define an object of the Dataloader class for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx3SmtksADVv"
      },
      "source": [
        "val_dataloader = DataLoader(coco_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAcd6ms9Ablw"
      },
      "source": [
        "Let's extract a mini-batch from val_dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzz5YqPLAeqE"
      },
      "source": [
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "for imgs_batch, target_batch, path_batch in val_dataloader:\r\n",
        "  break\r\n",
        "\r\n",
        "print(imgs_batch.shape)\r\n",
        "print(target_batch.shape, target_batch.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NtX0hCrBLPO"
      },
      "source": [
        "## Creating a YOLO-v3 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbphNJZ1BMTL"
      },
      "source": [
        "The YOLO-v3 network is built of convolutional layers with stride 2, skip connections, and up-sampling layers. There are no pooling layers. The network receives an image whose size is 416*416 as input and provides three YOLO outputs.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/yolo-v3.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The network down-samples the input image by a factor of 32 to a feature map of\r\n",
        "size `13*13`, where `yolo-out1` is provided. To improve the detection performance, the `13*13` feature map is up-sampled to `26*26` and `52*52`, where we have `yolo-out2` and `yolo-out3`, respectively. A cell in a feature map predicts three bounding boxes that correspond to three predefined anchors. As a result, the network predicts `13*13*3+26*26*3+52*52*3=10647` bounding boxes in total.\r\n",
        "\r\n",
        "A bounding box is defined using 85 numbers:\r\n",
        "- Four coordinates, `[x, y, w, h]`\r\n",
        "- An abjectness score\r\n",
        "- `C=80` class predictions corresponding to 80 object categories in the COCO\r\n",
        "dataset\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35eMiedD2T-"
      },
      "source": [
        "### Parsing the configuration file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvq2OnyRD3Zd"
      },
      "source": [
        ""
      ]
    }
  ]
}