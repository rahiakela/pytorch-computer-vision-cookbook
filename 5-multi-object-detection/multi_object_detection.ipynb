{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multi-object-detection.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNrAaMIOohMXjCjwemCccmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/pytorch-computer-vision-cookbook/blob/main/5-multi-object-detection/multi_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYnu9PNv3NW"
      },
      "source": [
        "# Multi-Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8iTz06Xv96_"
      },
      "source": [
        "Object detection is the process of locating and classifying existing objects in an image. Identified objects are shown with bounding boxes in the image. There are two methods for general object detection: region proposal-based and regression/classification-based. \r\n",
        "\r\n",
        "In this notebook, we will use a regression/classification-based method called YOLO.we will learn how to implement the YOLO-v3 algorithm and train and\r\n",
        "deploy it for object detection using PyTorch.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b08H4QZ1x5Mh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERXmfyIgx6hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf46def-bfa8-4087-bad8-62ee3ff6f961"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torchvision.transforms.functional as TF\r\n",
        "from torchvision.transforms.functional import to_pil_image\r\n",
        "from torch import optim\r\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\r\n",
        "\r\n",
        "\r\n",
        "import torch\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(torch.__version__)\r\n",
        "\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "\r\n",
        "import copy\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pylab as plt\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kcx0rArMwN4K"
      },
      "source": [
        "## Creating datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8OY4pK8wPF4"
      },
      "source": [
        "We will need to download the COCO dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAX1F7s3yfDG"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "# Download the following GitHub repository\r\n",
        "git clone https://github.com/pjreddie/darknet\r\n",
        "\r\n",
        "# Create a folder named data\r\n",
        "mkdir data\r\n",
        "\r\n",
        "# copy the get_coco_dataset.sh file\r\n",
        "cp darknet/scripts/get_coco_dataset.sh data\r\n",
        "\r\n",
        "# execute the get_coco_dataset.sh file\r\n",
        "chmod 755 data/get_coco_dataset.sh\r\n",
        "./data/get_coco_dataset.sh\r\n",
        "\r\n",
        "# Create a folder named config\r\n",
        "mkdir data/config\r\n",
        "# copy the yolov3.cfg file\r\n",
        "cp darknet/cfg/yolov3.cfg data/config/\r\n",
        "\r\n",
        "# Finally, download the coco.names file and put it in the data folder\r\n",
        "wget https://github.com/pjreddie/darknet/blob/master/data/coco.names\r\n",
        "cp coco.names data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHVVh1wl5S8W"
      },
      "source": [
        "### Creating a custom COCO dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7n523yg5U9B"
      },
      "source": [
        "Now that we've downloaded the COCO dataset, we will create training and validation datasets and dataloaders using PyTorch's Dataset and Dataloader classes.\r\n",
        "\r\n",
        "we will define the CocoDataset class and show some sample images from\r\n",
        "the training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS3CdwTyz4kQ"
      },
      "source": [
        "class CocoDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, files_path, transform=None, trans_params=None):\r\n",
        "    # get list of images\r\n",
        "    with opne(files_path, \"r\") as file:\r\n",
        "      self.img_path = file.readlines()\r\n",
        "    # get list of labels\r\n",
        "    self.label_path = [path.replace(\"images\", \"labels\").replace(\".png\", \"txt\").replace(\".jpg\", \".txt\") for path in self.img_path]\r\n",
        "    self.trans_params = trans_params \r\n",
        "    self.transform = transform \r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.img_path)\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    img_path = self.img_path[index % len(self.img_path)].rstrip()\r\n",
        "    img = Image.open(img_path).convert(\"RGB\")\r\n",
        "    label_path = self.label_path[index % len(self.img_path)].rstrip()\r\n",
        "\r\n",
        "    labels = None\r\n",
        "    if os.path.exists(label_path):\r\n",
        "      labels = np.loadtxt(label_path).replace(-1, 5)\r\n",
        "    if self.transform:\r\n",
        "      img, labels = self.transform(img, labels, self.trans_params)\r\n",
        "\r\n",
        "    return img, labels, img_path"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtPZ47Zq58l1"
      },
      "source": [
        "Next, we will create an object of the CocoDataset class for the validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bla3nDy84ett"
      },
      "source": [
        "root_data = \"./data/coco\"\r\n",
        "train_file_path = os.path.join(root_data, \"trainvalno5k.txt\")\r\n",
        "coco_train = CocoDataset(train_file_path)\r\n",
        "print(len(coco_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQEhrvjA6l2r"
      },
      "source": [
        "# Get a sample item from coco_val:\r\n",
        "img, labels, img_path = coco_train[1] \r\n",
        "print(\"image size:\", img.size, type(img))\r\n",
        "print(\"labels shape:\", labels.shape, type(labels))\r\n",
        "print(\"labels \\n\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnikLO-r6zVh"
      },
      "source": [
        "Let's display a sample image from the coco_train and coco_val datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYeV0rUw60LL"
      },
      "source": [
        "val_file_path = os.path.join(root_data, \"5k.txt\")\r\n",
        "coco_val = CocoDataset(val_file_path, transform=None, trans_params=None)\r\n",
        "print(len(coco_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7fdGIVLBoAA"
      },
      "source": [
        "# Get a sample item from coco_val:\r\n",
        "img, labels, img_path = coco_val[7] \r\n",
        "print(\"image size:\", img.size, type(img))\r\n",
        "print(\"labels shape:\", labels.shape, type(labels))\r\n",
        "print(\"labels \\n\", labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE2Z2pSOB75K"
      },
      "source": [
        "Let's display a sample image from the coco_train and coco_val datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llLvc96UB8xN"
      },
      "source": [
        "# Get a list of COCO object names\r\n",
        "coco_names_path=\"./data/coco.names\"\r\n",
        "fp = open(coco_names_path, \"r\")\r\n",
        "coco_names = fp.read().split(\"\\n\")[:-1]\r\n",
        "print(\"number of classese:\", len(coco_names))\r\n",
        "print(coco_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_PWY1bhCUa0"
      },
      "source": [
        "# Define a rescale_bbox helper function to rescale normalized bounding boxes to the original image size\r\n",
        "def rescale_bbox(bb, W, H):\r\n",
        "  x, y, w, h = bb\r\n",
        "  return [x * W, y * H, w * W, h * H]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZZT7wK3Cwlo"
      },
      "source": [
        "# Define the show_img_bbox helper function to show an image with object bounding boxes\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "# if the font that's passed to ImageFont.truetype is not available\r\n",
        "# Alternatively, you may use a more common font\r\n",
        "# fnt = ImageFont.truetype('arial.ttf', 16)\r\n",
        "fnt = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 16)\r\n",
        "\r\n",
        "def show_img_bbox(img, targets):\r\n",
        "  if torch.is_tensor(img):\r\n",
        "      img=to_pil_image(img)\r\n",
        "  if torch.is_tensor(targets):\r\n",
        "      targets=targets.numpy()[:,1:]\r\n",
        "      \r\n",
        "  W, H=img.size\r\n",
        "  draw = ImageDraw.Draw(img)\r\n",
        "  \r\n",
        "  for target in targets:\r\n",
        "      id_=int(target[0])\r\n",
        "      bbox=target[1:]\r\n",
        "      bbox=rescale_bbox(bbox,W,H)\r\n",
        "      xc, yc, w, h=bbox\r\n",
        "      \r\n",
        "      color = [int(c) for c in COLORS[id_]]\r\n",
        "      name=coco_names[id_]\r\n",
        "      \r\n",
        "      draw.rectangle(((xc-w/2, yc-h/2), (xc+w/2, yc+h/2)), outline=tuple(color), width=3)\r\n",
        "      draw.text((xc-w/2, yc-h/2), name, font=fnt, fill=(255, 255, 255, 0))\r\n",
        "  plt.imshow(np.array(img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGHUB83DVdE"
      },
      "source": [
        "# Call the show_img_bbox helper function to show a sample image from coco_train\r\n",
        "np.random.seed(2)\r\n",
        "rnd_ind=np.random.randint(len(coco_train))\r\n",
        "img, labels, img_path = coco_train[rnd_ind] \r\n",
        "print(img.size, labels.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "show_img_bbox(img, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTnFFEmEJBO"
      },
      "source": [
        "# Call the show_img_bbox helper function to show a sample image from coco_val\r\n",
        "np.random.seed(0)\r\n",
        "rnd_ind=np.random.randint(len(coco_val))\r\n",
        "img, labels, img_path = coco_val[rnd_ind] \r\n",
        "print(img.size, labels.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "show_img_bbox(img, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57j3VGL1EIea"
      },
      "source": [
        "### Transforming data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx1yM6tZEcMM"
      },
      "source": [
        "In this section, we will define a transform function and the parameters to be passed to the CocoDataset class.\r\n",
        "\r\n",
        "In the Transforming the data subsection, we defined the functions required for data transformation. These transformations were required to resize images, augment the data, or convert the data into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8yfXZqiwXaG"
      },
      "source": [
        "# First, we will define a pad_to_square helper function\r\n",
        "def pad_to_square(img, boxes, pad_value=0, normalized_labels=True):\r\n",
        "  \"\"\"\r\n",
        "  img: A PIL image\r\n",
        "  boxes: A numpy array with a shape of (n, 5) that contains n bounding boxes\r\n",
        "  pad_value: The pixel fill value, which defaults to zero\r\n",
        "  normalized_labels: A flag to show whether the bounding boxes were normalized to the range [0, 1]\r\n",
        "  \"\"\"\r\n",
        "  w, h = img.size\r\n",
        "  w_factor, h_factor = (w, h) if normalized_labels else (1, 1)\r\n",
        "\r\n",
        "  # calculate the padding size and divided it into two values: pad1 and pad2\r\n",
        "  dim_diff = np.abs(h - w)\r\n",
        "  pad1 = dim_diff // 2\r\n",
        "  pad2 = dim_diff - pad1\r\n",
        "\r\n",
        "  if h <= w:\r\n",
        "    left, top, right, bottom = 0, pad1, 0, pad2\r\n",
        "  else:\r\n",
        "    left, top, right, bottom = pad1, 0, pad2, 0\r\n",
        "  padding = (left, top, right, bottom)\r\n",
        "\r\n",
        "  # calculate the padding size on each side of the image\r\n",
        "  img_padded = TF.pad(img, padding=padding, fill=pad_value)\r\n",
        "  w_padded, h_padded = img_padded.size\r\n",
        "\r\n",
        "  # adjust the bounding box coordinates based on the padding size.\r\n",
        "  x1 = w_factor * (boxes[:, 1] - boxes[:, 3] / 2)\r\n",
        "  y1 = h_factor * (boxes[:, 2] - boxes[:, 4] / 2)\r\n",
        "  x2 = w_factor * (boxes[:, 1] + boxes[:, 3] / 2)\r\n",
        "  y2 = h_factor * (boxes[:, 2] + boxes[:, 4] / 2)\r\n",
        "\r\n",
        "  # Then, we adjusted x1, y1, x2, y2 by adding the padding sizes.\r\n",
        "  x1 += padding[0]   # left\r\n",
        "  y1 += padding[1]   # top\r\n",
        "  x2 += padding[2]   # right\r\n",
        "  y2 += padding[3]   # bottom\r\n",
        "\r\n",
        "  # calculate the bounding boxes using the adjusted values of x1, y1, x2, y2.\r\n",
        "  # Note that we normalized the labels again to the range of [0, 1].\r\n",
        "  boxes[:, 1] = ((x1 + x2) / 2) / w_padded\r\n",
        "  boxes[:, 2] = ((y1 + y2) / 2) / h_padded\r\n",
        "  boxes[:, 3] *= w_factor / w_padded\r\n",
        "  boxes[:, 4] *= h_factor / h_padded\r\n",
        "\r\n",
        "  return img_padded, boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GifVuwxT5da9"
      },
      "source": [
        "# Define the hflip helper function to horizontally flip images\r\n",
        "def hflip(image, labels):\r\n",
        "  image = TF.hflip(image)\r\n",
        "  labels[:, 1] = 1.0 - labels[:, 1]\r\n",
        "\r\n",
        "  return image, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgr2HH0Z6EvB"
      },
      "source": [
        "# Define the transformer function\r\n",
        "def transformer(image, labels, params):\r\n",
        "  \"\"\"\r\n",
        "  image: A PIL image\r\n",
        "  labels: Bounding boxes as a numpy array that's (n, 5) in size\r\n",
        "  params: A Python dictionary containing the transformation parameters\r\n",
        "  \"\"\"\r\n",
        "  if params[\"pad2square\"] is True:\r\n",
        "    image, labels = pad_to_square(image, labels)\r\n",
        "  image = TF.resize(image, params[\"target_size\"])\r\n",
        "\r\n",
        "  if random.random() < params[\"p_hflip\"]:\r\n",
        "    image, labels = hflip(image, labels)   # randomly flip the image for data augmentation\r\n",
        "\r\n",
        "  image = TF.to_tensor(image)              # convert the PIL image into a PyTorch tensor\r\n",
        "  targets = torch.zeros((len(labels), 6))  # also convert into a PyTorch tensor of size n*6. The extra dimension will be used to index images in a mini-batch.\r\n",
        "  targets[:, 1:] = torch.from_numpy(labels)\r\n",
        "\r\n",
        "  return image, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTa1PS-38yCu"
      },
      "source": [
        "We redefined `coco_train`; however, this time, we passed transformer and\r\n",
        "`trans_params_train` to the CocoDataset class. To force the horizontal flip, we set the `p_hflip` probability to 1.0. In practice, we usually set the probability to 0.5. You can see the effect of the transformations on the sample image. The image has been zero-padded from the top and bottom, resized to 416*416, and horizontally flipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIo2I-xZ9E8P"
      },
      "source": [
        "# Now, let's create an object of CocoDataset for training data by passing the transformer\r\n",
        "trans_params_train = {\r\n",
        "    \"target_size\": (416, 416),\r\n",
        "    \"pad2square\": True,\r\n",
        "    \"p_hflip\": 1.0,\r\n",
        "    \"normalized_labels\": True\r\n",
        "}\r\n",
        "\r\n",
        "coco_train = CocoDataset(train_file_path, transform=transformer, trans_params=trans_params_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHRU4Mq9AlfS"
      },
      "source": [
        "np.random.seed(2)\r\n",
        "rnd_ind = np.random.randint(len(coco_train))\r\n",
        "img, targets, img_path = coco_train[rnd_ind]\r\n",
        "print(\"image shape:\", img.shape)\r\n",
        "print(\"labels shape:\", targets.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "show_img_bbox(img,targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHgRd-DQ9KqQ"
      },
      "source": [
        "Similarly, we redefined coco_val. We did not need data augmentation for the\r\n",
        "validation data, so we set the probability of p_hflip to 0.0. Check out the transformed sample size. It has been zero-padded from the top and bottom and resized to 416*416 but not flipped."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LxQkQnA9OFX"
      },
      "source": [
        "# Similarly, we will define an object of CocoDataset by passing the transformer to validate the data\r\n",
        "trans_params_val = {\r\n",
        "    \"target_size\": (416, 416),\r\n",
        "    \"pad2square\": True,\r\n",
        "    \"p_hflip\": 0.0,\r\n",
        "    \"normalized_labels\": True\r\n",
        "}\r\n",
        "\r\n",
        "coco_val = CocoDataset(val_file_path, transform=transformer, trans_params=trans_params_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jz9uH6TBjPG"
      },
      "source": [
        "np.random.seed(2)\r\n",
        "rnd_ind = np.random.randint(len(coco_val))\r\n",
        "img, targets, img_path = coco_val[rnd_ind]\r\n",
        "print(\"image shape:\", img.shape)\r\n",
        "print(\"labels shape:\", targets.shape)\r\n",
        "\r\n",
        "plt.rcParams['figure.figsize'] = (20, 10)\r\n",
        "COLORS = np.random.randint(0, 255, size=(80, 3),dtype=\"uint8\")\r\n",
        "show_img_bbox(img,targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTsLgmr0Bu7-"
      },
      "source": [
        "### Defining the Dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij0t73SaB0WC"
      },
      "source": [
        "We will define two dataloaders for training and validation of datasets so we\r\n",
        "can get mini-batches of data from coco_train and coco_val.\r\n",
        "\r\n",
        "We also defined the collate_fn function to process a mini-batch and return\r\n",
        "PyTorch tensors. The function was given as an argument to the Dataloader class so that the process happens on the fly. In the function, we grouped the images, targets, and paths in the mini-batch using zip(*iterateble). Then, we removed any empty bounding boxes in the targets. Next, we set the sample index in the mini-batch. Finally, we concatenated the images and targets as PyTorch tensors. To see how this works, we extracted a mini-batch from train_dl and val_dl and printed the shape of the returned tensors.\r\n",
        "\r\n",
        "Define an object of the Dataloader class for the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPPoXhIZ-I_C"
      },
      "source": [
        "batch_size = 8\r\n",
        "\r\n",
        "def collate_fn(batch):\r\n",
        "  imgs, targets, paths = list(zip(*batch))\r\n",
        "\r\n",
        "  # Remove empty boxes\r\n",
        "  targets = [boxes for boxes in targets if boxes is not None]\r\n",
        "\r\n",
        "  # set the sample index\r\n",
        "  for b_i, boxes in enumerate(targets):\r\n",
        "    boxes[:, 0] = b_i\r\n",
        "  targets = torch.cat(targets, 0)\r\n",
        "  imgs = torch.stack([img for img in imgs])\r\n",
        "\r\n",
        "  return imgs, targets, paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RPdwMIp--BS"
      },
      "source": [
        "train_dataloader = DataLoader(coco_train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsH6qhr1_aBK"
      },
      "source": [
        "Let's extract a mini-batch from train_dataloader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b92tAyO_cUc"
      },
      "source": [
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "for imgs_batch, target_batch, path_batch in train_dataloader:\r\n",
        "  break\r\n",
        "\r\n",
        "print(imgs_batch.shape)\r\n",
        "print(target_batch.shape, target_batch.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz7cYPVuACbg"
      },
      "source": [
        "Define an object of the Dataloader class for the validation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx3SmtksADVv"
      },
      "source": [
        "val_dataloader = DataLoader(coco_val, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAcd6ms9Ablw"
      },
      "source": [
        "Let's extract a mini-batch from val_dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzz5YqPLAeqE"
      },
      "source": [
        "torch.manual_seed(0)\r\n",
        "\r\n",
        "for imgs_batch, target_batch, path_batch in val_dataloader:\r\n",
        "  break\r\n",
        "\r\n",
        "print(imgs_batch.shape)\r\n",
        "print(target_batch.shape, target_batch.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NtX0hCrBLPO"
      },
      "source": [
        "## Creating a YOLO-v3 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbphNJZ1BMTL"
      },
      "source": [
        "The YOLO-v3 network is built of convolutional layers with stride 2, skip connections, and up-sampling layers. There are no pooling layers. The network receives an image whose size is 416*416 as input and provides three YOLO outputs.\r\n",
        "\r\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/object-detection-images/yolo-v3.png?raw=1' width='800'/>\r\n",
        "\r\n",
        "The network down-samples the input image by a factor of 32 to a feature map of\r\n",
        "size `13*13`, where `yolo-out1` is provided. To improve the detection performance, the `13*13` feature map is up-sampled to `26*26` and `52*52`, where we have `yolo-out2` and `yolo-out3`, respectively. A cell in a feature map predicts three bounding boxes that correspond to three predefined anchors. As a result, the network predicts `13*13*3+26*26*3+52*52*3=10647` bounding boxes in total.\r\n",
        "\r\n",
        "A bounding box is defined using 85 numbers:\r\n",
        "- Four coordinates, `[x, y, w, h]`\r\n",
        "- An abjectness score\r\n",
        "- `C=80` class predictions corresponding to 80 object categories in the COCO\r\n",
        "dataset\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35eMiedD2T-"
      },
      "source": [
        "### Parsing the configuration file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvq2OnyRD3Zd"
      },
      "source": [
        "We need to parse the configuration file to be able to build the model. We have provided a `myutils.py` file that contains a helper function with which you can do this. The configuration file `yolov3.cfg` was downloaded previously in the Creating Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEaQDvhlzycD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86d279c-2dd2-4bff-9fc2-d78c4b1f1384"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahiakela/pytorch-computer-vision-cookbook/main/5-multi-object-detection/myutils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-25 09:16:02--  https://raw.githubusercontent.com/rahiakela/pytorch-computer-vision-cookbook/main/5-multi-object-detection/myutils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6302 (6.2K) [text/plain]\n",
            "Saving to: ‘myutils.py’\n",
            "\n",
            "myutils.py          100%[===================>]   6.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-25 09:16:02 (44.0 MB/s) - ‘myutils.py’ saved [6302/6302]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5eFYpck1_p4"
      },
      "source": [
        "from myutils import parse_model_config, create_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYKM4fz31CAe"
      },
      "source": [
        "config_path = \"./config/yolov3.cfg\"\r\n",
        "blocks_list = parse_model_config(config_path)\r\n",
        "blocks_list[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33Zfvid01jTN"
      },
      "source": [
        "### Creating PyTorch modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJiHoUYf1k-H"
      },
      "source": [
        "we will create PyTorch modules based on our parsed configuration file.\r\n",
        "\r\n",
        "Now, let's call the create_layers function and get a list of PyTorch modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCIKaaqO2Jlu"
      },
      "source": [
        "hyper_parameters, model_list = create_layers(blocks_list)\r\n",
        "print(model_list),\r\n",
        "print(hyper_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-YMojuY2spK"
      },
      "source": [
        "### Defining the Darknet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4CSFBLC2uSS"
      },
      "source": [
        "Now, let's learn how to define the Darknet class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R27Y8rop3ehH"
      },
      "source": [
        "class Darknet(torch.nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, config_path, img_size=416):\r\n",
        "    super(Darknet, self).__init__()\r\n",
        "    self.blocks_list = parse_model_config(config_path)\r\n",
        "    self.hyperparameters, self.module_list = create_layers(self.blocks_list)\r\n",
        "    self.img_size = img_size\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    img_dim = x.shape[2]\r\n",
        "    layer_outputs, yolo_outputs = [], []\r\n",
        "\r\n",
        "    for block, module in zip(self.blocks_list[1:], self.module_list):\r\n",
        "      if block[\"type\"] in [\"convolutional\", \"upsample\", \"maxpool\"]:\r\n",
        "         x = module(x)\r\n",
        "      elif block[\"type\"] == \"shortcut\":\r\n",
        "        layer_ind = int(block[\"from\"])\r\n",
        "        x = layer_outputs[-1] + layer_outputs[layer_ind]\r\n",
        "      elif block[\"type\"] == \"yolo\":\r\n",
        "        x = module[0](x)\r\n",
        "        yolo_outputs.append(x)\r\n",
        "      elif block[\"type\"] == \"route\":\r\n",
        "        x = torch.cat([layer_outputs[int(l_i)] for l_i in block[\"layers\"].split(\",\")], 1)\r\n",
        "\r\n",
        "      layer_outputs.append(x)\r\n",
        "\r\n",
        "    yolo_out_cat = torch.cat(yolo_outputs, 1)\r\n",
        "\r\n",
        "    return yolo_out_cat, yolo_outputs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k94QT98FNR2z"
      },
      "source": [
        "Let's create an object of the Darknet class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXpYFt-NNRY5"
      },
      "source": [
        "model = Darknet(config_path).to(device)\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPE9yGswP3uo"
      },
      "source": [
        "print(next(model.parameters()).to(device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mVEO6o8QAbY"
      },
      "source": [
        "Next, let's test the model using a dummy input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDKsRMCsQBM4"
      },
      "source": [
        "dummy_img = torch.rand(1, 3, 416, 416).to(device)\r\n",
        "with torch.no_grad():\r\n",
        "  dummy_out_cat, dummy_out = model.forward(dummy_img)\r\n",
        "  print(dummy_out_cat.shape)\r\n",
        "  print(dummy_out[0].shape, dummy_out[1].shape, dummy_out[2].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZBw6kNcQ70j"
      },
      "source": [
        "### Defining the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz-xvR5NQ9JF"
      },
      "source": [
        "we will define a loss function for the YOLO-v3 architecture. To get an insight\r\n",
        "into of the YOLO-v3 loss, recall that the model output comprises the following elements:\r\n",
        "\r\n",
        "- [x, y, w, h] of bounding boxes\r\n",
        "- An objectness score\r\n",
        "- Class predictions for 80 object categories\r\n",
        "\r\n",
        "Thus, the YOLO-v3 loss function is composed of the following:\r\n",
        "\r\n",
        "$$ loss = loss_x + loss_y + loss_w + loss_h + loss_{obj} + loss_{cls}$$\r\n",
        "\r\n",
        "Here, we have the following:\r\n",
        "\r\n",
        "- $loss_x, loss_y, loss_w, loss_h$ are the mean squared error of x, y, w, h\r\n",
        "- $loss_{obj}$ is the binary cross-entropy loss of the objectness score\r\n",
        "- $loss_{cls}$ is the binary cross-entropy loss of class predictions\r\n",
        "\r\n",
        "Here, we will learn how to implement a combined loss function for the YOLO-v3\r\n",
        "algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrcwOxQixamY"
      },
      "source": [
        "def get_loss_batch(output, targets, params_loss, opt=None):\r\n",
        "  \"\"\"\r\n",
        "  output: A list of three tensors corresponding to the YOLO-v3 outputs.\r\n",
        "  targets: The ground truth, a tensor of shape n*6, where n is the total number of bounding boxes in the batch.\r\n",
        "  params_loss: A Python dict, which contains the loss parameters.\r\n",
        "  opt: An object of the optimizer class. The default value is None.\r\n",
        "  \"\"\"\r\n",
        "  # extract the parameters from the params_loss dictionary\r\n",
        "  ignore_thres = params_loss[\"ignore_thres\"]\r\n",
        "  scaled_anchors = params_loss[\"scaled_anchors\"]\r\n",
        "  mse_loss = params_loss[\"mse_loss\"]\r\n",
        "  bce_loss = params_loss[\"bce_loss\"]\r\n",
        "\r\n",
        "  num_yolos = params_loss[\"num_yolos\"]\r\n",
        "  num_anchors = params_loss[\"num_anchors\"]\r\n",
        "  obj_scale = params_loss[\"obj_scale\"]\r\n",
        "  noobj_scale = params_loss[\"noobj_scale\"]\r\n",
        "\r\n",
        "  # In each iteration, we extracted the YOLO output (yolo_out), the number of bounding boxes (num_bboxs), and the grid size(gird_size)\r\n",
        "  loss = 0.0\r\n",
        "  for yolo_ind in range(num_yolos):\r\n",
        "    yolo_out = output[yolo_ind]\r\n",
        "    batch_size, num_bbxs, _ = yolo_out.shape\r\n",
        "\r\n",
        "    # get grid size\r\n",
        "    gz_2 = num_bbxs / num_anchors\r\n",
        "    grid_size = int(np.sqrt(gz_2))\r\n",
        "\r\n",
        "    yolo_out = yolo_out.view(batch_size, num_anchors, grid_size, grid_size, -1)\r\n",
        "\r\n",
        "    # extracting the predicted bounding boxes\r\n",
        "    pred_boxes = yolo_out[:, :, :, :, :4]\r\n",
        "    x, y, w, h = transform_bbox(pred_boxes, scaled_anchors[yolo_ind])\r\n",
        "    pred_conf = yolo_out[:, :, :, :, 4]\r\n",
        "    pred_cls_prob = yolo_out[:, :, :, :, 5:]\r\n",
        "\r\n",
        "    yolo_targets = get_yolo_targets({\r\n",
        "        \"pred_cls_prob\": pred_cls_prob,\r\n",
        "        \"pred_boxes\":pred_boxes,    \r\n",
        "        \"targets\": targets,    \r\n",
        "        \"anchors\": scaled_anchors[yolo_ind],    \r\n",
        "        \"ignore_thres\": ignore_thres,\r\n",
        "    })\r\n",
        "\r\n",
        "    obj_mask=yolo_targets[\"obj_mask\"]        \r\n",
        "    noobj_mask=yolo_targets[\"noobj_mask\"]            \r\n",
        "    tx=yolo_targets[\"tx\"]                \r\n",
        "    ty=yolo_targets[\"ty\"]                    \r\n",
        "    tw=yolo_targets[\"tw\"]                        \r\n",
        "    th=yolo_targets[\"th\"]                            \r\n",
        "    tcls=yolo_targets[\"tcls\"]                                \r\n",
        "    t_conf=yolo_targets[\"t_conf\"]\r\n",
        "\r\n",
        "    # calculate the mean-squared error between the predicted and the target coordinates of the bounding boxes\r\n",
        "    loss_x = mse_loss(x[obj_mask], tx[obj_mask])\r\n",
        "    loss_y = mse_loss(y[obj_mask], ty[obj_mask])\r\n",
        "    loss_w = mse_loss(w[obj_mask], tw[obj_mask])\r\n",
        "    loss_h = mse_loss(h[obj_mask], th[obj_mask])\r\n",
        "\r\n",
        "    # calculate the binary cross-entropy loss between the predicted and target objectness scores.\r\n",
        "    loss_conf_obj = bce_loss(pred_conf[obj_mask], t_conf[obj_mask])\r\n",
        "    loss_conf_noobj = bce_loss(pred_conf[noobj_mask], t_conf[noobj_mask])\r\n",
        "    loss_conf = obj_scale * loss_conf_obj + noobj_scale * loss_conf_noobj\r\n",
        "    loss_cls = bce_loss(pred_cls_prob[obj_mask], tcls[obj_mask])\r\n",
        "    # Finally, all the calculated loss values were summed together.\r\n",
        "    loss += loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\r\n",
        "\r\n",
        "  # If the optimization object, has a value, we compute the gradients and perform the\r\n",
        "  # optimization step; otherwise, we return the total loss.\r\n",
        "  if opt is not None:\r\n",
        "    opt.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    opt.step\r\n",
        "\r\n",
        "  return loss.item()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZM8ewpx0rOe"
      },
      "source": [
        "def transform_bbox(bbox, anchors):\r\n",
        "  \"\"\"\r\n",
        "  This function takes the predicted bounding boxes and transforms them so that they're\r\n",
        "  compatible with the target values. This transformation is the reverse\r\n",
        "  of the transform_outputs function from the YOLOLayer class.\r\n",
        "\r\n",
        "  bbox: A tensor of shape (batch_size, 3, grid_size, grid_size, 4) that contains the predicted bounding boxes.\r\n",
        "  anchors: A tensor of shape (3, 2) that contains the scaled widths and heights of the three anchors for each YOLO output.\r\n",
        "  \"\"\"\r\n",
        "  # extract the x, y, w, h tensors from bbox.\r\n",
        "  x=bbox[:,:,:,:,0]\r\n",
        "  y=bbox[:,:,:,:,1]\r\n",
        "  w=bbox[:,:,:,:,2]\r\n",
        "  h=bbox[:,:,:,:,3]\r\n",
        "  # Then, we sliced the two columns of the anchors tensor and reshaped them into tensors of shape(1, 3, 1, 1)\r\n",
        "  anchor_w = anchors[:, 0].view((1, 3, 1, 1))\r\n",
        "  anchor_h = anchors[:, 1].view((1, 3, 1, 1))       \r\n",
        "  \r\n",
        "  # Next, we transformed the values and returned a list of four tensors corresponding to x, y, w, h.\r\n",
        "  x=x-x.floor()\r\n",
        "  y=y-y.floor()\r\n",
        "  w= torch.log(w / anchor_w + 1e-16)\r\n",
        "  h= torch.log(h / anchor_h + 1e-16)\r\n",
        "\r\n",
        "  return x, y, w, h"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITOgLrDk0168"
      },
      "source": [
        "def get_yolo_targets(params):\r\n",
        "  \"\"\"\r\n",
        "  pred_boxes: A tensor of shape (batch_size, 3, grid_size, grid_size, 4) that contains the predicted bounding boxes.\r\n",
        "  pred_cls_prob: A tensor of shape (batch_size, 3, grid_size, grid_size, 80) that contains the predicted class probabilities.\r\n",
        "  targets: A tensor of shape (n, 6), where n is the number of bounding boxes in a batch, containing the ground truth bounding boxes and labels.\r\n",
        "  anchors: A tensor of shape (2, 3) that contains the scaled width and height of the three anchors.\r\n",
        "  ignore_thres: A scalar float value set to 0.5, which is used as the threshold value.\r\n",
        "  \"\"\"\r\n",
        "  pred_boxes=params[\"pred_boxes\"]\r\n",
        "  pred_cls_prob=params[\"pred_cls_prob\"]\r\n",
        "  target=params[\"targets\"]\r\n",
        "  anchors=params[\"anchors\"] \r\n",
        "  ignore_thres=params[\"ignore_thres\"] \r\n",
        "\r\n",
        "  batch_size = pred_boxes.size(0)\r\n",
        "  num_anchors = pred_boxes.size(1)\r\n",
        "  grid_size = pred_boxes.size(2)\r\n",
        "  num_cls = pred_cls_prob.size(-1)\r\n",
        "\r\n",
        "  # nitializing the output tensors\r\n",
        "  sizeT=batch_size, num_anchors, grid_size, grid_size\r\n",
        "  obj_mask = torch.zeros(sizeT,device=device,dtype=torch.uint8)\r\n",
        "  noobj_mask = torch.ones(sizeT,device=device,dtype=torch.uint8)\r\n",
        "  tx = torch.zeros(sizeT, device=device, dtype=torch.float32)\r\n",
        "  ty= torch.zeros(sizeT, device=device, dtype=torch.float32)\r\n",
        "  tw= torch.zeros(sizeT, device=device, dtype=torch.float32)\r\n",
        "  th= torch.zeros(sizeT, device=device, dtype=torch.float32)\r\n",
        "\r\n",
        "  # Then, we sliced the target bounding boxes from the target tensor and scaled by the grid size. \r\n",
        "  target_bboxes = target[:, 2:] * grid_size\r\n",
        "  t_xy = target_bboxes[:, :2]\r\n",
        "  t_wh = target_bboxes[:, 2:]\r\n",
        "  t_x, t_y = t_xy.t()\r\n",
        "  t_w, t_h = t_wh.t()\r\n",
        "\r\n",
        "  grid_i, grid_j = t_xy.long().t()\r\n",
        "\r\n",
        "  # Next, we calculated the intersection over union (IoU) of a target and the three anchors\r\n",
        "  iou_with_anchors = [get_iou_WH(anchor, t_wh) for anchor in anchors]\r\n",
        "  iou_with_anchors = torch.stack(iou_with_anchors)\r\n",
        "  # Then, we found the anchor that has the highest IOU with a target.\r\n",
        "  best_iou_wa, best_anchor_ind = iou_with_anchors.max(0)\r\n",
        "\r\n",
        "  # setting the object mask tensors\r\n",
        "  batch_inds, target_labels = target[:, :2].long().t()\r\n",
        "  obj_mask[batch_inds, best_anchor_ind, grid_j, grid_i] = 1\r\n",
        "  noobj_mask[batch_inds, best_anchor_ind, grid_j, grid_i] = 0\r\n",
        "\r\n",
        "  for ind, iou_wa in enumerate(iou_with_anchors.t()):\r\n",
        "    noobj_mask[batch_inds[ind], iou_wa > ignore_thres, grid_j[ind], grid_i[ind]] = 0\r\n",
        "  \r\n",
        "  # setting x and y\r\n",
        "  tx[batch_inds, best_anchor_ind, grid_j, grid_i] = t_x - t_x.floor()\r\n",
        "  ty[batch_inds, best_anchor_ind, grid_j, grid_i] = t_y - t_y.floor()\r\n",
        "\r\n",
        "  # setting w and h\r\n",
        "  anchor_w=anchors[best_anchor_ind][:, 0]\r\n",
        "  tw[batch_inds, best_anchor_ind, grid_j, grid_i] = torch.log(t_w / anchor_w + 1e-16)\r\n",
        "  \r\n",
        "  anchor_h=anchors[best_anchor_ind][:, 1]\r\n",
        "  th[batch_inds, best_anchor_ind, grid_j, grid_i] = torch.log(t_h / anchor_h + 1e-16)\r\n",
        "  \r\n",
        "  # setting target classes\r\n",
        "  tcls[batch_inds, best_anchor_ind, grid_j, grid_i, target_labels] = 1\r\n",
        "\r\n",
        "  output = {\r\n",
        "      \"obj_mask\" : obj_mask,\r\n",
        "      \"noobj_mask\" : noobj_mask,\r\n",
        "      \"tx\": tx,\r\n",
        "      \"ty\": ty,\r\n",
        "      \"tw\": tw,\r\n",
        "      \"th\": th,\r\n",
        "      \"tcls\": tcls,\r\n",
        "      \"t_conf\": obj_mask.float(),\r\n",
        "  }\r\n",
        "\r\n",
        "  return output"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drKWFAyS5dzB"
      },
      "source": [
        "def get_iou_WH(wh1, wh2):\r\n",
        "  wh2 = wh2.t()\r\n",
        "  w1, h1 = wh1[0], wh1[1]\r\n",
        "  w2, h2 = wh2[0], wh2[1]\r\n",
        "  inter_area = torch.min(w1, w2) * torch.min(h1, h2)\r\n",
        "  union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\r\n",
        "  \r\n",
        "  return inter_area / union_area"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvQUc800B-9y"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdG5rwrlCA3W"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXn8sLqq5miu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}